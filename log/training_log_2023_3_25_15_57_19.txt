
#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################
 

This is the configuration used by this training:
Configuration name: 3d_fullres
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [32, 256, 256], 'median_image_size_in_voxels': [51.0, 2030.0, 2031.5], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization', 'ZScoreNormalization', 'ZScoreNormalization'], 'use_mask_for_norm': [False, False, False], 'UNet_class_name': 'PlainConvUNet', 'UNet_base_num_features': 32, 'n_conv_per_stage_encoder': [2, 2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2, 2], 'num_pool_per_axis': [3, 6, 6], 'pool_op_kernel_sizes': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2], [1, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'unet_max_num_features': 320, 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'batch_dice': True} 
 
These are the global plan.json settings:
 {'dataset_name': 'Dataset036_Glom', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [51, 2030, 2032], 'image_reader_writer': 'Tiff3DIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 4095.0, 'mean': 1400.635009765625, 'median': 225.0, 'min': 0.0, 'percentile_00_5': 25.0, 'percentile_99_5': 4095.0, 'std': 1582.2962646484375}, '1': {'max': 4095.0, 'mean': 247.52272033691406, 'median': 232.0, 'min': 22.0, 'percentile_00_5': 102.0, 'percentile_99_5': 540.0, 'std': 85.7035140991211}, '2': {'max': 4095.0, 'mean': 327.4246826171875, 'median': 311.0, 'min': 0.0, 'percentile_00_5': 138.0, 'percentile_99_5': 798.0, 'std': 111.63261413574219}}} 
 
2023-03-25 15:57:20.518772: unpacking dataset... 
2023-03-25 15:57:21.563224: unpacking done... 
2023-03-25 15:57:21.565455: do_dummy_2d_data_aug: True 
2023-03-25 15:57:21.592924: Unable to plot network architecture: 
2023-03-25 15:57:21.592924: No module named 'hiddenlayer' 
2023-03-25 15:57:21.614837:  
2023-03-25 15:57:21.614837: Epoch 0 
2023-03-25 15:57:21.614837: Current learning rate: 0.01 
2023-03-25 16:00:38.819422: train_loss -0.2442 
2023-03-25 16:00:38.820419: val_loss -0.5187 
2023-03-25 16:00:38.821415: Pseudo dice [0.774, 0.8855, 0.8515] 
2023-03-25 16:00:38.822412: Epoch time: 197.2 s 
2023-03-25 16:00:38.823409: Yayy! New best EMA pseudo Dice: 0.837 
2023-03-25 16:00:41.036132:  
2023-03-25 16:00:41.036132: Epoch 1 
2023-03-25 16:00:41.037145: Current learning rate: 0.00999 
2023-03-25 16:03:18.808874: train_loss -0.5776 
2023-03-25 16:03:18.816678: val_loss -0.6561 
2023-03-25 16:03:18.818180: Pseudo dice [0.8622, 0.9049, 0.8773] 
2023-03-25 16:03:18.819179: Epoch time: 157.77 s 
2023-03-25 16:03:18.819179: Yayy! New best EMA pseudo Dice: 0.8414 
2023-03-25 16:03:22.092859:  
2023-03-25 16:03:22.093861: Epoch 2 
2023-03-25 16:03:22.094853: Current learning rate: 0.00998 
2023-03-25 16:06:06.643918: train_loss -0.6772 
2023-03-25 16:06:06.648515: val_loss -0.7396 
2023-03-25 16:06:06.655998: Pseudo dice [0.9093, 0.9262, 0.9016] 
2023-03-25 16:06:06.663980: Epoch time: 164.55 s 
2023-03-25 16:06:06.665488: Yayy! New best EMA pseudo Dice: 0.8485 
2023-03-25 16:06:09.435246:  
2023-03-25 16:06:09.435246: Epoch 3 
2023-03-25 16:06:09.436419: Current learning rate: 0.00997 
2023-03-25 16:08:44.086959: train_loss -0.7296 
2023-03-25 16:08:44.090502: val_loss -0.764 
2023-03-25 16:08:44.092494: Pseudo dice [0.9176, 0.9247, 0.9188] 
2023-03-25 16:08:44.093493: Epoch time: 154.65 s 
2023-03-25 16:08:44.093493: Yayy! New best EMA pseudo Dice: 0.8557 
2023-03-25 16:08:47.310185:  
2023-03-25 16:08:47.310185: Epoch 4 
2023-03-25 16:08:47.311654: Current learning rate: 0.00996 
2023-03-25 16:11:22.626992: train_loss -0.754 
2023-03-25 16:11:22.629423: val_loss -0.7894 
2023-03-25 16:11:22.663883: Pseudo dice [0.9232, 0.9293, 0.9309] 
2023-03-25 16:11:22.673361: Epoch time: 155.32 s 
2023-03-25 16:11:22.682858: Yayy! New best EMA pseudo Dice: 0.8629 
2023-03-25 16:11:26.085813:  
2023-03-25 16:11:26.087028: Epoch 5 
2023-03-25 16:11:26.087028: Current learning rate: 0.00995 
2023-03-25 16:14:02.648441: train_loss -0.7784 
2023-03-25 16:14:02.694782: val_loss -0.8042 
2023-03-25 16:14:02.743730: Pseudo dice [0.9283, 0.9339, 0.9337] 
2023-03-25 16:14:02.789460: Epoch time: 156.56 s 
2023-03-25 16:14:02.820928: Yayy! New best EMA pseudo Dice: 0.8698 
2023-03-25 16:14:05.426710:  
2023-03-25 16:14:05.426710: Epoch 6 
2023-03-25 16:14:05.428012: Current learning rate: 0.00995 
2023-03-25 16:16:37.730565: train_loss -0.7863 
2023-03-25 16:16:37.734064: val_loss -0.8133 
2023-03-25 16:16:37.735094: Pseudo dice [0.9303, 0.9376, 0.9429] 
2023-03-25 16:16:37.736094: Epoch time: 152.31 s 
2023-03-25 16:16:37.737098: Yayy! New best EMA pseudo Dice: 0.8765 
2023-03-25 16:16:40.801689:  
2023-03-25 16:16:40.802685: Epoch 7 
2023-03-25 16:16:40.802685: Current learning rate: 0.00994 
2023-03-25 16:19:17.709941: train_loss -0.7938 
2023-03-25 16:19:17.748880: val_loss -0.8181 
2023-03-25 16:19:17.787360: Pseudo dice [0.9326, 0.9377, 0.9362] 
2023-03-25 16:19:17.800843: Epoch time: 156.91 s 
2023-03-25 16:19:17.825754: Yayy! New best EMA pseudo Dice: 0.8824 
2023-03-25 16:19:20.352966:  
2023-03-25 16:19:20.353961: Epoch 8 
2023-03-25 16:19:20.354466: Current learning rate: 0.00993 
2023-03-25 16:22:02.090180: train_loss -0.8006 
2023-03-25 16:22:02.123653: val_loss -0.8232 
2023-03-25 16:22:02.165194: Pseudo dice [0.9333, 0.9393, 0.9389] 
2023-03-25 16:22:02.166693: Epoch time: 161.74 s 
2023-03-25 16:22:02.167695: Yayy! New best EMA pseudo Dice: 0.8879 
2023-03-25 16:22:04.824873:  
2023-03-25 16:22:04.825872: Epoch 9 
2023-03-25 16:22:04.825872: Current learning rate: 0.00992 
2023-03-25 16:24:52.952133: train_loss -0.8037 
2023-03-25 16:24:52.953633: val_loss -0.8322 
2023-03-25 16:24:52.954637: Pseudo dice [0.9361, 0.9411, 0.9399] 
2023-03-25 16:24:52.955634: Epoch time: 168.13 s 
2023-03-25 16:24:52.956630: Yayy! New best EMA pseudo Dice: 0.893 
2023-03-25 16:24:56.039696:  
2023-03-25 16:24:56.041142: Epoch 10 
2023-03-25 16:24:56.042164: Current learning rate: 0.00991 
2023-03-25 16:27:33.431452: train_loss -0.8089 
2023-03-25 16:27:33.432955: val_loss -0.8306 
2023-03-25 16:27:33.447562: Pseudo dice [0.9366, 0.939, 0.9351] 
2023-03-25 16:27:33.449555: Epoch time: 157.39 s 
2023-03-25 16:27:33.449555: Yayy! New best EMA pseudo Dice: 0.8974 
2023-03-25 16:27:36.442068:  
2023-03-25 16:27:36.443065: Epoch 11 
2023-03-25 16:27:36.443065: Current learning rate: 0.0099 
2023-03-25 16:30:20.777851: train_loss -0.8135 
2023-03-25 16:30:20.781837: val_loss -0.8395 
2023-03-25 16:30:20.783337: Pseudo dice [0.9388, 0.9405, 0.9486] 
2023-03-25 16:30:20.784339: Epoch time: 164.34 s 
2023-03-25 16:30:20.785345: Yayy! New best EMA pseudo Dice: 0.9019 
2023-03-25 16:30:23.956748:  
2023-03-25 16:30:23.956748: Epoch 12 
2023-03-25 16:30:23.957745: Current learning rate: 0.00989 
2023-03-25 16:33:01.843220: train_loss -0.8163 
2023-03-25 16:33:01.845217: val_loss -0.8405 
2023-03-25 16:33:01.845217: Pseudo dice [0.9386, 0.9407, 0.9431] 
2023-03-25 16:33:01.846214: Epoch time: 157.89 s 
2023-03-25 16:33:01.848207: Yayy! New best EMA pseudo Dice: 0.9058 
2023-03-25 16:33:05.094331:  
2023-03-25 16:33:05.095330: Epoch 13 
2023-03-25 16:33:05.096327: Current learning rate: 0.00988 
2023-03-25 16:35:46.172035: train_loss -0.8127 
2023-03-25 16:35:46.212554: val_loss -0.8367 
2023-03-25 16:35:46.263627: Pseudo dice [0.9394, 0.9385, 0.9451] 
2023-03-25 16:35:46.332494: Epoch time: 161.08 s 
2023-03-25 16:35:46.375959: Yayy! New best EMA pseudo Dice: 0.9093 
2023-03-25 16:35:49.504437:  
2023-03-25 16:35:49.505434: Epoch 14 
2023-03-25 16:35:49.505434: Current learning rate: 0.00987 
2023-03-25 16:38:20.954907: train_loss -0.8189 
2023-03-25 16:38:20.973887: val_loss -0.8428 
2023-03-25 16:38:20.989166: Pseudo dice [0.9421, 0.9414, 0.9424] 
2023-03-25 16:38:20.991507: Epoch time: 151.45 s 
2023-03-25 16:38:20.997017: Yayy! New best EMA pseudo Dice: 0.9126 
2023-03-25 16:38:23.861658:  
2023-03-25 16:38:23.861658: Epoch 15 
2023-03-25 16:38:23.862676: Current learning rate: 0.00986 
2023-03-25 16:40:57.044076: train_loss -0.8273 
2023-03-25 16:40:57.065080: val_loss -0.8519 
2023-03-25 16:40:57.087552: Pseudo dice [0.9432, 0.9422, 0.9506] 
2023-03-25 16:40:57.105539: Epoch time: 153.19 s 
2023-03-25 16:40:57.129778: Yayy! New best EMA pseudo Dice: 0.9159 
2023-03-25 16:40:59.847554:  
2023-03-25 16:40:59.847554: Epoch 16 
2023-03-25 16:40:59.848445: Current learning rate: 0.00986 
2023-03-25 16:43:37.071965: train_loss -0.8254 
2023-03-25 16:43:37.072962: val_loss -0.8465 
2023-03-25 16:43:37.074299: Pseudo dice [0.9418, 0.9418, 0.9456] 
2023-03-25 16:43:37.074299: Epoch time: 157.23 s 
2023-03-25 16:43:37.075299: Yayy! New best EMA pseudo Dice: 0.9186 
2023-03-25 16:43:39.543608:  
2023-03-25 16:43:39.543608: Epoch 17 
2023-03-25 16:43:39.544606: Current learning rate: 0.00985 
2023-03-25 16:46:11.427998: train_loss -0.8277 
2023-03-25 16:46:11.428994: val_loss -0.8497 
2023-03-25 16:46:11.430496: Pseudo dice [0.9431, 0.9451, 0.9427] 
2023-03-25 16:46:11.431509: Epoch time: 151.89 s 
2023-03-25 16:46:11.431509: Yayy! New best EMA pseudo Dice: 0.9211 
2023-03-25 16:46:13.753160:  
2023-03-25 16:46:13.754156: Epoch 18 
2023-03-25 16:46:13.754156: Current learning rate: 0.00984 
2023-03-25 16:48:47.122986: train_loss -0.8313 
2023-03-25 16:48:47.130478: val_loss -0.8553 
2023-03-25 16:48:47.138488: Pseudo dice [0.9448, 0.9453, 0.9498] 
2023-03-25 16:48:47.159495: Epoch time: 153.37 s 
2023-03-25 16:48:47.164997: Yayy! New best EMA pseudo Dice: 0.9237 
2023-03-25 16:48:50.253761:  
2023-03-25 16:48:50.253761: Epoch 19 
2023-03-25 16:48:50.254768: Current learning rate: 0.00983 
2023-03-25 16:51:31.271228: train_loss -0.8344 
2023-03-25 16:51:31.295204: val_loss -0.8523 
2023-03-25 16:51:31.301965: Pseudo dice [0.9459, 0.9461, 0.9422] 
2023-03-25 16:51:31.302965: Epoch time: 161.02 s 
2023-03-25 16:51:31.304959: Yayy! New best EMA pseudo Dice: 0.9258 
2023-03-25 16:51:33.852899:  
2023-03-25 16:51:33.852899: Epoch 20 
2023-03-25 16:51:33.854167: Current learning rate: 0.00982 
2023-03-25 16:54:07.339619: train_loss -0.8301 
2023-03-25 16:54:07.340635: val_loss -0.8568 
2023-03-25 16:54:07.355104: Pseudo dice [0.9467, 0.9458, 0.9498] 
2023-03-25 16:54:07.359599: Epoch time: 153.49 s 
2023-03-25 16:54:07.361616: Yayy! New best EMA pseudo Dice: 0.9279 
2023-03-25 16:54:10.406461:  
2023-03-25 16:54:10.407458: Epoch 21 
2023-03-25 16:54:10.407458: Current learning rate: 0.00981 
2023-03-25 16:56:43.797657: train_loss -0.8317 
2023-03-25 16:56:43.818139: val_loss -0.8529 
2023-03-25 16:56:43.823647: Pseudo dice [0.9448, 0.943, 0.9479] 
2023-03-25 16:56:43.830130: Epoch time: 153.39 s 
2023-03-25 16:56:43.838406: Yayy! New best EMA pseudo Dice: 0.9297 
2023-03-25 16:56:47.087573:  
2023-03-25 16:56:47.088569: Epoch 22 
2023-03-25 16:56:47.088569: Current learning rate: 0.0098 
2023-03-25 16:59:40.655297: train_loss -0.8313 
2023-03-25 16:59:40.679256: val_loss -0.8556 
2023-03-25 16:59:40.694243: Pseudo dice [0.9446, 0.9438, 0.9514] 
2023-03-25 16:59:40.698230: Epoch time: 173.57 s 
2023-03-25 16:59:40.699226: Yayy! New best EMA pseudo Dice: 0.9314 
2023-03-25 16:59:43.636708:  
2023-03-25 16:59:43.637706: Epoch 23 
2023-03-25 16:59:43.637706: Current learning rate: 0.00979 
2023-03-25 17:02:22.903321: train_loss -0.8357 
2023-03-25 17:02:22.909215: val_loss -0.8603 
2023-03-25 17:02:22.944217: Pseudo dice [0.9452, 0.9455, 0.9487] 
2023-03-25 17:02:22.970341: Epoch time: 159.27 s 
2023-03-25 17:02:22.976325: Yayy! New best EMA pseudo Dice: 0.9329 
2023-03-25 17:02:26.148769:  
2023-03-25 17:02:26.149801: Epoch 24 
2023-03-25 17:02:26.149801: Current learning rate: 0.00978 
2023-03-25 17:05:15.489691: train_loss -0.8342 
2023-03-25 17:05:15.519195: val_loss -0.8614 
2023-03-25 17:05:15.536472: Pseudo dice [0.9469, 0.9461, 0.9491] 
2023-03-25 17:05:15.540973: Epoch time: 169.34 s 
2023-03-25 17:05:15.555471: Yayy! New best EMA pseudo Dice: 0.9343 
2023-03-25 17:05:20.043905:  
2023-03-25 17:05:20.043905: Epoch 25 
2023-03-25 17:05:20.045022: Current learning rate: 0.00977 
